=========================================
ЛР1: Введение в разработку модулей ядра
=========================================

Название директории для лабораторной работы: ``lab1_kernel_development_introduction``

.. _pkgs-lab1:

Необходимые пакеты (названия приведены для Arch-подобных дистрибутивов):

- Группа пакетов ``base-devel`` (включает в себя пакеты 
  ``autoconf``
  ``automake``
  ``binutils``
  ``bison``
  ``fakeroot``
  ``file``
  ``findutils``
  ``flex``
  ``gawk``
  ``gcc``
  ``gettext``
  ``grep``
  ``groff``
  ``gzip``
  ``libtool``
  ``m4``
  ``make``
  ``pacman``
  ``patch``
  ``pkgconf``
  ``sed``
  ``sudo``
  ``texinfo``
  ``which``
  ). По сути это основные инструменты сборки. На многие из них полагается система сборки ядра.

- Некоторые дополнительные служебные утилиты: ``cpio`` (потребуется в процессе создания initramfs-образа для ядра)
  и ``bc`` (калькулятор, требуется системе сборки ядра).
  
  .. kmod libelf pahole xmlto python-sphinx python-sphinx_rtd_theme graphviz imagemagick
  
- QEMU (пакеты ``qemu`` и ``qemu-arch-extra``). Это мультиплатформенный эмулятор и гипервизор.
  
  Зачем нам нужен QEMU? Если *пользовательский* процесс не отвечает или, скажем, выполнил недопустимую
  операцию (например, обратился к неаллоцированному адресу памяти), ядро может принудительно закрыть его.
  Другие процессы продолжат свою работу, как ни в чем не бывало.
  
  Но мы разрабатываем модули ядра и при их подключении, они линкуются к ядру и
  становятся с ним, в неком роде, одним целым. Что будет, если такого рода ошибка
  возникает в коде ядра? Хоть ядро и спроектировано как можно более надежным и
  попытается восстановиться при возникновении ошибки модуля ядра, при разработке
  модулей ядра, ошибки часто приводят к его падению и к невозможности дальше
  продолжать нормальную работу до перезагрузки.
  
  Конечно, мы могли бы разрабатывать модули ядра непосредственно на хост-системе,
  но это неудобно и небезопасно (что, если ваш модуль случайно обратиться к зоне памяти
  файловой системы и вызовет повреждение какого-нибудь важного файла).
  Более того, отлаживать ядро с использованием традиционных инструментов, таких как
  GDB не так просто, как пользовательские приложения.
  
  Если запускать GDB отладку на одной машине, это по сути будет "отладкой ядра с
  использованием отлаживаемого кода ядра". Поэтому, на практике часто применяют две
  машины, соединенные между собой. На одной машине выполняется отлаживаемое ядро, 
  а другая выполняет роль управляющей. 
  Другой альтернативой является QEMU. У него есть встроенный интерфейс GDB, что
  позволяет выполнять отладку на самом низком уровне.
  
  Таким образом, QEMU очень удобен при разработке ядра и практически незаменим.

На Arch-подобных системах установить все сразу можно, выполнив

.. code-block::
   
   sudo pacman -S base-devel qemu qemu-arch-extra bc cpio



Краткие теоретические сведения
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Прежде всего, Linux — это не операционная система, а ядро. Дистрибутивы операционных систем,
использующие ядро Linux принято называть объединенно GNU/Linux. Они, в свою очередь, включают
в себя Linux-ядро, Unix-подобное user space окружение и, зачастую, менеджер пакетов.
Unix-подобное окружение предоставляет набор "стандартных" команд таких, как 
``ls``, ``cat``, ``cp``, ..., а также другие системные инструменты (вроде библиотек, 
демонов, подсистем), на которые, в свою очередь, полагаются пользовательские приложения.

Unix-like окружения были настолько различны, что в определенный момент были разработаны
спецификации `LSB (Linux Standard Base) <https://en.wikipedia.org/wiki/Linux_Standard_Base>`__
для того, чтобы прикладные программы могли работать на всех LSB-совместимых дистрибутивах.

С точки зрения ядра, на работающей машине все программные компоненты делятся на kernel space
(относящиеся непосредственно к Linux-ядру: модули ядра, его подсистемы, драйвера устройств)
и user space (все, кроме ядра, использующее его  интерфейсы: демоны, пользовательские
программы и другие процессы).


Сборка минимального Linux-ядра вручную
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Ядро Linux – монолитное, модульное и очень гибкое. Перед сборкой ядра, необходимо выполнить
его конфигурацию. За конфигурацию отвечает инструмент Kconfig, входящий в дерево исходников
ядра. Есть множество опций и инструментов конфигурации:

- Путем непосредственного редактирования файла конфигурации ``.config``. Это удобно для
  применения внешних инструментов сборки, переноса конфигов, их сравнения между собой и т.д.
- Используя конфигурацию "по-умолчанию", поставляемую с релизом ядра. Такая конфигурация
  содержит умеренно-резонные значения и является хорошей стартовой точкой для дальнейшей
  кастомизации (например, создателями дистрибутивов). Осуществляется посредством
  ``make defconfig``.
- Используя старый файл конфигурации ``.config``. Полезно, если при обновлении ядра на более
  новую версию, необходимо сохранить старые значения тех параметров, которые не изменились.
  Осуществляется посредством ``make oldconfig``.
- **Используя псевдографический интерфейс настройки**. У ядра много, очень много
  параметров настройки. Поэтому, удобно делать это именно в псевдографическом или графическом
  интерфейсе. В нем все настройки ядра структурированы и разбиты по категориям. Можно легко
  получить справку по тому или иному пункту конфигурации, выполнить поиск, отследить зависимости
  определенного пункта конфигурации от других пунктов. Осуществляется посредством
  ``make menuconfig`` (или ``make nconfig`` – менее предпочтительно).
  Является наиболее распространенным способом конфигурации вручную.
- Используя полноценный графический интерфейс. Осуществляется посредством ``make xconfig``
  или ``make gconfig``. На деле псевдографический интерфейс оказывается удобнее графического.
- Используя наиболее "легковесную" конфигурацию. Осуществляется посредством
  ``make tinyconfig``. Настраивает ядро таким образом, чтобы оно было как можно более легким
  (содержало как можно меньше включенных компонентов и имело как можно меньший футпринт
  в оперативной памяти).

Другие опции конфигурации можно посмотреть, вызвав ``make help``. Там же будут выведены и
остальные команды системы сборки ядра Kbuild.

В любом случае, конфигурация является файлом ``.config``, который порождает тот или иной способ
конфигурации. Поэтому, можно например сперва выполнить ``make tinyconfig`` для получения
наиболее легковесного ядра, а затем выполнить ``make menuconfig`` и тонко подстроить ядро,
отталкиваясь от tinyconfig конфигурации.

Процедура:

#. Убедиться, что установлены все `необходимые пакеты <#pkgs-lab1>`_.
#. Перейти на `kernel.org <https://kernel.org>`__ и выкачать tarball последней stable (**не EOL**)
   ветки ядра.
#. Распаковать его в директорию, в которой мы еще некоторое время будем вести разработку:
   
   .. code-block:: bash
   
      ## create directory 'kernel' in /home/user/
      mkdir -p ~/kernel
      ## extract .tar.xz kernel image to ~/kernel/
      tar -xJvf linux-*.tar.xz -C ~/kernel/
   
#. Сконфигурировать ядро. Включение опций множества компонентов (например, включение сборки 
   большого числа доступных модулей ядра) существенно увеличивают время его компиляции. Поэтому,
   будем использовать основанную на "ванильной" (vanilla — дефолтной) конфигурацию ядра, которая
   является относительно легковесной и, в то же время, не является слишком урезанной.
   
   .. легковесной и урезанной конфигурации allnoconfig.
   
   Выполните:
   
   #. ``make distclean`` – в случае, если необходимо вернуть дерево исходников ядра в первозданное
      состояние (например, удалить все сгенерированные файлы, в том числе конфигурации, а также
      удалить патчи).
   
   #. ``make defconfig``. Это породит vanilla-конфигурацию ``.config``. Ее возьмем за основу.
      
   .. #. ``make allnoconfig``. Это породит конфигурацию ``.config`` по-умолчанию, без сборки
   ..    модулей ядра, довольно быструю в компиляции. Ее возьмем за основу.
      
   #. ``make menuconfig``. Отредактируем полученный конфиг.
   
   Необходимо установить следующие опции:
   
   * ``[*] 64-bit kernel`` – yes.
   * ``General Setup --->``
     
     * ``Preemption Model ---> Voluntary Kernel Preemption (Desktop)``
     * ``[*] Initial RAM filesystem and RAM disk (initramfs/initrd) support`` – yes
     * ``Configure standard kernel features (expert users)``
       
       * ``[*] Enable support for printk`` – yes
       * ``[*] BUG() support`` – yes
   
   * ``Executable file formats --->``
     
     * ``[*] Kernel support for ELF binaries`` – yes
     * ``[*] Kernel support for scripts starting with #!`` – yes
   
   * ``Device drivers --->``
     
     * ``Generic Driver Options --->``
       
       * ``[*] Maintain a devtmpfs filesystem to mount at /dev`` – yes
       * ``[*]   Automount devtmpfs at /dev, after the kernel mounted the rootfs`` – yes
     
     * ``Character devices --->``
       
       * ``[*] Enable TTY`` – yes
       * ``Serial drivers --->``
       
         * ``[*] 8250/16550 and compatible serial support`` – yes
         * ``[*]   Console on 8250/16550 and compatible serial port`` – yes
   
   * ``File systems --->``
     
     * ``Pseudo filesystems --->``
       
       * ``[*] /proc file system support`` – yes
       * ``[*] sysfs file system support`` – yes
   
   * ``Kernel hacking --->``
     
     * ``Compile-time checks and compiler option --->``
       
       * ``[*] Compile the kernel with debug info`` – yes
       * ``[*]   Provide GDB scripts for kernel debugging`` – yes
   
   Затем сохранить конфиг с именем по-умолчанию ``.config`` и выйти из menuconfig
   
#. Запустить сборку ядра.

   Для сборки ядра и модулей необходимо выполнить ``make -jN``, где N – количество
   доступных ядер + 1 (для параллельной компиляции и, следовательно, ее ускорения).
   Посмотреть количество ядер можно, выполнив ``nproc``.  Например, для 4 ядер:
   ``make -j5``.
   
   На данном этапе нам не требуется сборка модулей ядра, поэтому время компиляции
   можно сократить, собрав только ядро, не запуская сборку модулей.
   Выполните: ``make -jN vmlinux`` – это соберет только ядро.
   Время сборки составляет примерно 40 минут на моей машине.
   
   (если хотите попробовать быстрее – используйте ``make allnoconfig``
   на предыдущем этапе, но ядро будет очень сильно урезанным).
   
   В результате появится собранный бинарник (образ) ядра ``vmlinux``.
   Давайте оценим его размер:
   
   .. code-block:: bash
      
      ls -hal ./
      
      # result:
      # -rwxr-xr-x   1 thd users 142M Sep  2 20:49 vmlinux
   
   Таким образом, бинарный образ ядра в данной конфигурации занимает 142 МиБ.
   Исторически, не все архитектуры могли хранить и загружать ядро такого размера,
   поэтому появился сжатый формат bzImage (и аналоги). Более подробно об этом
   можно почитать `тут <https://en.wikipedia.org/wiki/Vmlinux>`__.
   
   Собранный ``vmlinux`` можно грузить с помощью QEMU напрямую, но давайте
   создадим также сжатый образ bzImage. Выполните:
   
   .. code-block:: bash
      
      # remember that N should be replaced as described above
      make -jN bzImage
   
   В конце сборки, вы увидите:
   
   .. code-block::
      
      Setup is 13820 bytes (padded to 13824 bytes).
      System is 2331 kB
      CRC fa499bd
      Kernel: arch/x86/boot/bzImage is ready  (#1)
   
   Оценим размер теперь:
   
   .. code-block:: bash
      
      ls -hal ./arch/x86/boot/bzImage
      
      # result:
      # -rw-r--r-- 1 thd users 2.3M Sep  2 20:50 ./arch/x86/boot/bzImage
   
   2.3 МиБ выглядит куда лучше. Особенно это полезно для embedded-систем, где
   ресурсы ограничены.

#. Можно создать символические ссылки на собранные образы для удобства, положив вне дерева исходников ядра:
   
   .. code-block:: bash
      
      ln -s ~/kernel/linux-*/vmlinux ~/kernel/vmlinux
      ln -s ~/kernel/linux-*/arch/x86/boot/bzImage ~/kernel/bzImage
      
   (будьте аккуратны с путями, символическую ссылку можно создать и на несуществующий объект)
   
#. Загрузить собранный образ (лежит в ``arch/x86/boot/bzImage``, или по созданному вами симлинку ``bzImage``),
   используя QEMU:
   
   .. code-block:: bash
      
      qemu-system-x86_64 -enable-kvm -m 256M -smp 4 -kernel "arch/x86_64/boot/bzImage" -append "console=ttyS0"
   
   Аргумент ``-m 256M`` указывает, что небходимо выделить 256 МиБ эмулируемой оперативной памяти
   (по-умолчанию выделяется 128 МиБ).
   
   Аргумент ``-smp 4`` говорит о том, что эмулируемой системе будет видно до 4 ядер хост-системы
   
   В результате можно увидеть сообщение:
     
     Kernel panic - not syncing: No working init found.  Try passing init= option to kernel.
   
   На данном этапе это нормально. Мы создадим initrd далее.


Примечания:

* Использовать отдельное окно QEMU с эмуляцией графического видеоадаптера неудобно.
  Намного лучше подключить вывод консоли ядра непосредственно в терминал.
  Для этого мы собирали ядро с поддержкой serial-интерфейса, который умеет эмулировать QEMU
  и который можно перенаправить в терминал. Для этого, QEMU необходимо запустить с аргументами
  ``-nographic`` и ``-append "console=ttyS0"`` (передает Linux-ядру параметр, говорящий, что
  консоль необходимо подключить к последовательному порту 0):
  
  .. code-block:: bash
     
     qemu-system-x86_64 -enable-kvm -m 256M -smp 4 -kernel "./bzImage" -append "console=ttyS0" -nographic
  
* Включение KVM позволяет ускорить виртуализацию и практически приблизить ее (> 90%) к
  производительности выполнения на физической машине. Тем не менее, на некоторых машинах
  (в основном, виртуальных), KVM недоступен. Поэтому, можно запускать без аргумента 
  ``-enable-kvm`` или использовать аргумент ``-no-kvm``:
  
  .. code-block:: bash
     
     qemu-system-x86_64 -no-kvm -m 256M -smp 4 -kernel "./bzImage" -append "console=ttyS0" -nographic
  
* Для выхода из QEMU можно использовать комбинацию < Ctrl + a >  < x > (сначала < Ctrl + a >, затем  < x >).
  Полный список комбинаций доступен по < Ctrl + a >  < h >



Сборка минимального Linux-окружения с использованием BusyBox. Initramfs
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

На предыдущем этапе, мы запустили собранное ядро в QEMU и обнаружили сообщение ошибки об отсутствующем init.
Это связано с тем, что ядро по окончании собственной инициализации, запускает процесс init и передает ему
управление. Init должен далее выполнить все необходимые инициализации userland (например, примонтировать
диски, запустить демоны, настроить сеть и т.д.).

Каким образом будет происходить загрузка системы, если, скажем корневая файловая система (rootfs, она же корень ``/``)
находится на каком-нибудь удаленном устройстве (например, сетевой диск), или же для дальнейшей работы системы необходимо
загрузить модуль ядра драйвера определенного устройства? Как в таком случае будет происходить загрузка?

Для решения этих проблем существует механизм initramfs. При загрузке ядра, загрузчик может передать ядру некий минимальный
образ файловой системы, который будет содержать все необходимое для дальнейшей загрузки (например, модули ядра устройства или
утилиты, выполняющие необходимую конфигурацию для доступа к сетевым дискам). Образ initramfs (ramdisk) будет загружен в
оперативную память и подмонтирован в качестве файловой системы, откуда сможет выполниться ``/init`` и провести необходимые
инициализации. Обычно initramfs выполняет монтирование корневого раздела ``/`` (rootfs) и передает управление дальше.

Busybox — это гибкий и конфигурируемый минималистический набор базовых user space утилит (``cp``, ``ls``, ``mv``, ``sh``, ...),
реализованный в виде одного исполняемого файла. Это позволяет добиться крайне малого размера. Как устроен BusyBox?
При исполнении программы, она получает 0м аргументом имя, под которым была запущена. К примеру, ``./myprog -flag somedir`` получит
нулевым аргументом ``./myprog``, а затем уже ``-flag`` и ``somedir``. BusyBox использует эту особенность. Он представляет собой
один исполняемый файл, на который созданы символические ссылки (symlink) для каждой из утилит. Таким образом, при вызове отдельных
утилит, запускается на исполнение один и тот же файл, но получает при этом разный 0й аргумент и, взависимости от его значения,
реализует конкретный функционал той или иной утилиты.

Мы могли бы собрать BusyBox и поместить сразу в rootfs диска, который ядро сможет вычитать сразу. Например, эмулируемого QEMU
SATA-диска с GPT таблицей разделов и root-разделом в формате ext4. Для этого наверняка пришлось бы включить пару опций в 
конфиге ядра (установить встраивание необходимых модулей SATA и EXT4 непосредственно в ядро, вместо сборки в качестве
отдельных подгружаемых модулей). Вполне допустимый вариант. Но мы пойдем другим путем и создадим initramfs с BusyBox.

Процедура создания ramdisk'a с BusyBox:

#. Убедиться, что установлены все `необходимые пакеты <#pkgs-lab1>`_.
#. Выгрузить BusyBox. Для расширения кругозора, будем использовать git-репозиторий.
   
   .. code-block:: bash
      
      ## get tags of remote repo without cloning it, select latest
      git ls-remote --tags --sort="v:refname" git://busybox.net/busybox.git | tail -1
      ## for example, ``refs/tags/1_32_0`` means latest version tag is ``1_32_0``
      ## we will refer to it as TAG
      
      ## shallow clone BBox git repo with specified tag to ~/kernel/busybox
      git clone -b TAG --depth=1 git://busybox.net/busybox.git ~/kernel/busybox
       
   Или можно выкачать tarball последней релиз-версии с 
   `HTTP-ресурса релизов BusyBox <https://busybox.net/downloads/?C=N;O=D>`__.
#. Интерфейс системы сборки BusyBox очень похож на систему сборки ядра. Настроим BusyBox.
   Выполните ``make menuconfig`` и установите:
   
   * ``Settings --->``
     
     * ``[*] Build static binary (no shared libs)`` – yes
   
   Это включит сборку BusyBox в виде статически линкованного бинарника.
#. Запустите сборку, выполнив ``make -jN``, где N – количество
   доступных ядер + 1 (для параллельной компиляции и, следовательно, ее ускорения).
   
   **Примечание**:
     На момент написания, в библиотеке glibc происходит переход от устаревшего ``libcrypt``
     к ``libxcrypt``. В результате, на новых Arch-подобных системах с ``glibc`` версии
     больше 2.31-2, статическая библиотека ``libcrypt.a`` недоступна.
     
     Поэтому, при сборке BusyBox, в make необходмо явно передавать ``CRYPT_AVAILABLE=n``,
     чтобы система сборки не пробовала линковаться с отсутствующей библиотекой.
     Например: ``make CRYPT_AVAILABLE=n -jN ...``.
     
     Вообще, сборку как ядра, так и других компонентов обычно выполняют с помощью
     тулчейна кросскомпиляции, который включает всё необходимое для целевой
     системы. На данном этапе, мы идем по упрощенному (и, конечно, не самому оптимальному)
     пути использования платформенного компилятора хост-системы.
     
     В дальнейшем мы рассмотрим системы автоматизированной сборки, которые, в том числе,
     собирают и необходимый тулчейн.
#. Запустите формирование результирующей директории (в терминологии BusyBox, "установка").
   
   .. code-block:: bash
      
      # add CRYPT_AVAILABLE=n if needed
      make -jN install
   
   В итоге, в заданной на этапе конфигурации директорию установки (по умолчанию, ``_install``
   внутри дерева исходников BusyBox) будет создан набор директорий; в ``/bin/busybox`` сброшен
   основной бинарник, а в других директориях – созданы символические ссылки на него для
   каждой из утилит. Далее буду использовать нотацию ``/`` как директории установки,
   поскольку она, в неком смысле, содержит дерево корневой системы.
#. Перейдите в результирующую директорию BusyBox. В ней мы создадим несколько необходимых
   директорий для монтирования procfs и sysfs — ``/proc`` и ``/sys``. Установим для них
   необходимые права доступа:
    
   .. code-block:: bash
      
      cd _install
      mkdir proc
      mkdir sys
      # 555, what does it mean ??
      chmod 555 proc
      chmod 555 sys
   
   Создадим файл ``/init``, который ядро выполнит сразу после окончания первоначальных
   этапов после запуска.
   
   Содержимое файла ``/init``:
   
   .. code-block:: bash
      
      #!/bin/sh
      
      mount -t proc none /proc
      mount -t sysfs none /sys
      
      # scans /sys for devices calling mknod to populate /dev
      mdev -s
      
      cat <<EOF
      
      ********* Boot took $(cut -d' ' -f1 /proc/uptime) s
      ********* Init done
      
      EOF
      
      exec /bin/sh
   
   Попробуйте объяснить, как работает этот скрипт.
   
   
   Поскольку ``/init`` должен быть исполняемым, необходимо установить флаг "executable":
   
   .. code-block:: bash
      
      chmod +x init
   
#. Создадим образ с полученной корневой директорией BusyBox, который затем будем использовать
   в качестве initramfs при загрузке ядра. Из директории установки BusyBox (``_install``),
   выполните:
   
   .. code-block:: bash
      
      find . -print0 | cpio --null --owner=0:0 -ov --format=newc | gzip -9 > ../../initramfs.cpio.gz
   
   В итоге, содержимое директории будет упаковано в cpio-архив, который затем будет сжат gzip и помещен
   двумя директориями выше (условно, в ``~/kernel`` – если соблюдали предложенную структуру директорий)
   под именем ``initramfs.cpio.gz``

#. Попробуем загрузить собранное ранее ядро с только что созданным BusyBox образом initramfs:
   
   .. code-block:: bash
      
      qemu-system-x86_64 -enable-kvm -m 256M -smp 4 -kernel "./bzImage" -initrd "./initramfs.cpio.gz" -append "console=ttyS0" -nographic
      
   Результат:
   
   .. code-block::
      
      [    2.003016] Freeing unused kernel image (text/rodata gap) memory: 2040K
      [    2.007398] Freeing unused kernel image (rodata/data gap) memory: 756K
      [    2.010724] Run /init as init process
      [    2.017627] mount (83) used greatest stack depth: 14568 bytes left
      
      ********* Boot took 2.03 s
      ********* Init done
      
      /bin/sh: can't access tty; job control turned off
      / # 
   
   Таким образом, мы запустили ядро и получили минимальный shell и набор утилит из комплекта BusyBox


Сборка первого модуля ядра
~~~~~~~~~~~~~~~~~~~~~~~~~~

Ядро Linux монолитное. Это значит, что, *условно*, все компоненты ядра лежат в одном адресном
пространстве и никак один от другого не изолированы. Такой подход делает ядро быстрым, поскольку
его компоненты могут легко обмениваться данными между собой. С этой особенностью связаны и
недостатки монолитных ядер: снижение надежности (один компонент при возникновении ошибки
может положить все ядро) и снижение безопасности (компоненты никак не изолированы друг
от друга и получение злоумышленником доступа к адресному пространству одного небезопасного
модуля ядра практически эквивалентно получению доступа ко всему ядру, а значит и
к любой составляющей работающей системы).

На заре компьютерной эпохи возникали споры о том, какое ядро лучше. Например, известный
`спор Таненбаума-Линюса <https://ru.wikipedia.org/wiki/Спор_Таненбаума_—_Торвальдса>`__.

Микроядро, в отличии от монолитного, выполняет самый минимальный набор функций,
обеспечивая планирование и переключение задач (scheduling), а также межпроцессовую
коммуникацию (IPC) между основными компонентами ядра, такими как
файловая система, драйвера устройств, сетевой стек и другими (называются серверами
в терминологии микроядер). И каждый компонент микроядра является изолированным, 
что дает большую надежность и безопасность.

Микроядра казались более современным вариантом в теории. Но в то время, когда микроядра
испытывали серьезные проблемы в разработке (коммуникация компонентов микроядра между
собой была очень медленной и неудобной), монолитное ядро Linux уже работало и работало
очень быстро. Его разработчики, в основном, Линюс приняли судьбоносное решение:
"Ядро должно быть быстрым и точка. Проблемы безопасности и надежности должны
решаться качественным, хорошо отлаженным кодом ядра. Если где-то в операционной
системе и есть место грязным хакам для достижения высокой производительности, то
это должно быть сделано в ядре. Чтобы пользовательские процессы уже работали в
чистом и надежном окружении".

Ядро Linux является модульным. Это значит, что компоненты ядра могут быть
динамически загружены и выгружены в рантайме примерно так, как плагины
в какой-нибудь программе. При загрузке модуля, ядро аллоцирует память,
выделяет для него ресурсы и прилинковывает модуль. Таким образом, модуль
как бы становится частью общего адресного пространства ядра. А при выгрузке
модуля – деаллоцирует и освобождает ресурсы. Если в работе модуля возникла
ошибка, ядро делает все возможное, чтобы восстановиться и продолжить работу,
за счет чего достигается более высокая отказоустойчивость, для этого
предусмотрено множество защитных механизмов. Но тем не менее,
в редких случаях, неправильно написанный модуль все равно может положить
всю систему и потребуется перезагрузка.


Пример модуля ядра
``````````````````
Рассмотрим и соберем простой модуль ядра, приведенный в директории `demo/lab1 <demo/lab1/>`__
репозитория. Модуль выводит приветственное и прощальное сообщение, а какже текущее значение
`jiffies <http://books.gigatux.nl/mirror/kerneldevelopment/0672327201/ch10lev1sec3.html>`__.

`firstmod.c <demo/lab1/firstmod.c>`__ :

.. code-block:: c
   
   #include <linux/module.h>	// required by all modules
   #include <linux/kernel.h>	// required for sysinfo
   #include <linux/init.h>    // used by module_init, module_exit macros
   #include <linux/jiffies.h>	// where jiffies and its helpers reside

   MODULE_DESCRIPTION("Basic module demo: init, deinit, printk, jiffies");
   MODULE_AUTHOR("thodnev");
   MODULE_VERSION("0.1");
   MODULE_LICENSE("Dual MIT/GPL");		// this affects the kernel behavior

   static int __init firstmod_init(void)
   {
            printk(KERN_INFO "Hello, $username!\njiffies = %lu\n", jiffies);
            return 0;
   }
    
   static void __exit firstmod_exit(void)
   {
            printk(KERN_INFO "Long live the Kernel!\n");
   }
    
   module_init(firstmod_init);
   module_exit(firstmod_exit);


В общем случае, модули ядра как правило выполняются не линейно, а с использованием callback-функций.

Callback-функции – это одна из наиболее широко применяемых концепций асинхронного программирования.
Вы могли раньше видеть их, например: в Cube HAL STM32; в мире браузерного JS и серверного Node.JS;
при разработке графических интерфейсов. Приведу два примера:

- Callback в GUI. Есть некая кнопка, по нажатию на которую должно происходить действие. Вместо того,
  чтобы постоянно проверять, была ли она нажата, разработчик регистрирует callback-функцию, которая
  будет выполнена как только пользователь нажмет эту кнопку;
  
- Cube HAL STM32. Вам нужно отправлять данные по UART. Вместо того, чтобы после записи в
  transmit-буфер данных дожидаться успешной отправки предыдущего байта перед отправкой нового,
  можно использовать прерывания. Прерывание отправит порцию данных и вызовет callback-функцию.
  Разработчик может выполнить в ней все, что хотел осуществить по наступлению события
  "отправка завершена".
  Подробности реализации для UART `на StackOverflow <https://stackoverflow.com/a/55628202/5750172>`__.

Таким образом, callback-функции вызываются при наступлении определенного события вместо того,
чтобы занимать процессорное время бесполезными опросами, и являются простым и ультрабыстрым
способом реализации концепции асинхронного программирования.


В примере выше есть две функции ``firstmod_init`` и ``firstmod_exit``, которые регистрируются
как callback (с помощью ``module_init`` и ``module_exit``) и будут вызываться, соответственно,
при загрузкеи выгрузке модуля ядра.

**Callback'и должны быть быстрыми!** Это главное правило. ``module_init`` -callback должен
быстро проинициализировать все необходимые для дальнейшей работы модуля ядра стуктуры данных,
установить callback-функции для реакции на нужные события, выполнить другие необходимые
инициализации и завершиться (функция, модуль ядра продолжает работу).
Если модулю ядра необходимо делать что-то в течение длительного
времени, это нужно делать, например запустив отдельный поток.

Остальная часть кода из примера должна быть более-менее понятна, поэтому, перейдем к сборке
этого примера, а затем вернемся к более детальному рассмотрению его работы.


Сборка модулей ядра
```````````````````
Для сборки модулей используется система сборки ядра KBuild. Она в значительной мере
полагается на make. Чтобы собрать модуль, достаточно создать простейший Makefile:

.. code-block:: make
   
   # (!) using paths with spaces may not work with Kbuild
   
   # this is what is used by Kbuild
   obj-m += firstmod.o
   
   # directory containing Makefile for kernel build
   KBUILDDIR ?= ../
   
  .PHONY: modules tidy
  
  # recur to the original kernel Makefile with some additions
  modules:
	        $(MAKE) -C "$(KBUILDDIR)" M="$(PWD)" modules
  
  tidy:
	        $(MAKE) -C "$(KBUILDDIR)" M="$(PWD)" clean

Здесь мы добавляем к переменной ``obj-m`` 
(как бы `append <https://www.gnu.org/software/make/manual/html_node/Appending.html>`__
к списку) название объектного файла модуля (.с заменили на .o), который хотим собрать.
Если модуль должен быть не отдельным файлом, подключаемым к ядру, а непосредственно
включен в образ ядра, то используется ``obj-y``.

И далее вызываем ``make modules`` в директории с деревом исходников ядра, задавая
переменную 
`M как директорию <https://www.kernel.org/doc/html/latest/kbuild/modules.html#options>`__,
где находится наш внешний (не входящий в само дерево исходников ядра) собираемый модуль.
Makefile ядра написан таким образом, что получая эту переменную, он перейдет в нужную
директорию и возьмет из Makefile, лежащего в ней, заданные переменные (нашу ``obj-m``)
и начнет сборку. Более подробно о том, как работают Makefile ядра можно почитать 
`по ссылке <https://www.kernel.org/doc/html/latest/kbuild/makefiles.html>`__, а о сборке
внешних модулей – `тут <https://www.kernel.org/doc/html/latest/kbuild/modules.html>`__.

Переменная ``KBUILDDIR`` является пользовательской, в ней в примере выше указана директория
с исходниками ядра. Если бы мы собирали отлаженный и уже готовый модуль ядра на
хост-системе, то ``KBUILDDIR`` можно было бы задать как 
``KBUILDDIR := /lib/modules/$(shell uname -r)/build/`` (стандартная директория на рабочей
системе, где хранятся нужные для сборки модулей ядра части Kbuild). Если интересно, можете
попробовать сделать это для модуля выше (потребуются заголовки ядра, в Arch-подобных системах
это пакет ``linux-headers`` для той версии ядра, которая у вас запущена). Но мы будем
разрабатывать модули ядра в QEMU, чтобы не ложить систему и не перезагружаться каждый раз
при возникновении ошибки при написании кода.

В директории `demo/lab1 <demo/lab1/>`__ лежит несколько измененный пример приведенного
выше `Makefile <demo/lab1/Makefile>`__, с минимальными отличиями. Для сборки модуля
ядра из приведенного примера выше, задайте ``KBUILDDIR`` равной директории, где
лежит дерево исходников ядра, использованное на предыдущих этапах. Это можно сделать
либо исправив переменную в самом Makefile, либо задав ее при вызове:

.. code-block:: bash
   
   # assuming Makefile is changed
   make modules
   
   # or provide the kernel source directory when calling make
   make KBUILDDIR="~/kernel/linux-5.8.7" modules

В результате сборки появится несколько файлов, главный из которых .ko – сам собранный
модуль. Попробуем загрузить его и рассмотрим его работу.


Работа с собранными модулями ядра в QEMU
````````````````````````````````````````
До этого момента, наше окружение в QEMU было изолированным. Но теперь есть необходимость
пробросить собранный модуль в запущенную в QEMU систему. Проще всего пробросить в QEMU
директорию, в которой он был собран. Это можно сделать несколькими спсобами:

- Подключить директорию с собранным модулем как сетевое хранилище. Это позволит читать
  и писать в нее прямо из работающей в QEMU системе. Такое хранилище будет работать
  довольно быстро, но потребует настройки сети как на хост-системе, так и на запущенной
  в QEMU гостевой;
  
- Создать из директории с собранным модулем образ, который подключить как виртуальный
  жесткий диск в QEMU. В образ можно как читать, так и писать. Но работа с ним на
  хост-системе несколько неудобна. Этот способ больше подходит для создания образов
  целых дисков, нежели для шаринга директории;

- Использовать VVFAT-диск в QEMU. QEMU содержит слой трансляции, позволяющий на лету
  создать из директории виртуальный жесткий диск. Это наиболее удобный способ для
  быстрого проброса директории в гостевую систему.
  Недостаток в том, что файловой системой виртуального диска будет устаревшая FAT,
  которая к тому же не умеет хранить полный набор прав доступа к файлам linux-систем.
  Диск можно будет читать (запись нестабильна), но для наших задач эти
  ограничения несущественны по сравнению с удобствами способа.

Используем последний способ.

#. Запустим QEMU, пробросив в него директорию с собранным модулем ядра как VVFAT-диск.
   
   .. code-block:: bash
      
      # here ./lab1 is our directory with built module
      qemu-system-x86_64 -enable-kvm -m 256M -smp 4 -kernel "./bzImage" -initrd "./initramfs.cpio.gz" \
                         -append "console=ttyS0" -nographic \
                         -drive file=fat:rw:./lab1,format=raw,media=disk
   
#. Проверим, что диск распознался нашим ядром.
   
   .. code-block:: shell
      
      ls -hal /dev
      
      # output
      # ....
      # brw-rw----    1 0        0           8,   0 Sep  2 05:59 sda
      # brw-rw----    1 0        0           8,   1 Sep  2 05:59 sda1
      # ....
   
   Видим, что в ``/dev`` присутствует два устройства: ``sda`` – наш диск (весь) и 
   ``sda1`` – его первый раздел в FAT32

#. Создадим директорию ``/mnt`` и смонтируем в нее первый раздел диска.
   
   .. code-block:: shell
      
      # create directory
      mkdir /mnt
      # you may also add mnt to initramfs to avoid creating it each time
      
      # mount qemu vvfat disk to it
      mount -t vfat /dev/sda1 /mnt
      
      # check that our files are present there
      ls -hal /mnt
      
      # output
      # ...
      # -rwxr-xr-x    1 0        0            937 Sep  2 10:09 Makefile
      # -rwxr-xr-x    1 0        0            698 Sep  2 09:55 firstmod.c
      # -rwxr-xr-x    1 0        0           4.5K Sep  2 10:20 firstmod.ko
      # ...
   
#. Загрузим собранный модуль и проверим
   
   .. code-block:: shell
      
      insmod /mnt/firstmod.ko
      
      # output
      # [  226.704469] firstmod: loading out-of-tree module taints kernel.
      # [  226.712031] Hello, $username!
      # [  226.712031] jiffies = 4294893705
   
   Таким образом, мы увидели вывод в лог ядра, порождаемый модулем в его ``init`` -функции,
   которая выполняется при загрузке модуля.
   
#. Проверим список загруженных модулей
   
   .. code-block:: shell
      
      lsmod
      
      # output
      # firstmod 16384 0 - Live 0xffffffffc00df000 (O)
   
   И видим, что наш модуль загружен и работает

#. Попробуем выгрузить модуль
   
   .. code-block:: shell
      
      rmmod firstmod
      
      # output
      # [  303.376667] Long live the Kernel!
      
      
      # check loaded modules again
      lsmod
      # output is empty – module was unloaded

   Мы выгрузили модуль. Функция ``exit`` модуля вывела в лог ядра сообщение.


Некоторые аспекты разработки модулей ядра
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
- Мы видели лог ядра по ходу выполнения. Он был смешан с выводом stdout, поскольку оба направлены в одно
  устройство. Отдельно посмотреть лог ядра можно, выполнив ``dmesg``:
  
  .. code-block:: bash
     
     dmesg | tail -10
  
  где ``tail -10`` – вывод 10 последних строк из stdin (знак ``|`` после ``dmesg`` означает, что его stdout
  необходимо передать на stdin команды справа). Мы используем ``tail``, поскольку лог ядра длинный.
  
- При загрузке модулю можно передавать аргументы. Более подробно в ``man insmod`` на хост-системе
  и в документации `LKMPG <https://www.tldp.org/LDP/lkmpg/2.6/html/x323.html>`__.

- В общем случае, модули могут зависеть друг от друга и должны загружаться в порядке, разрешающем
  зависимости между ними. Например, так делает ``modprobe``. Мы пока не затрагиваем этих аспектов
  и будем загружать и выгружать модули вручную.

- Информацию о модуле можно посмотреть, используя ``modinfo``, передав файл модуля ядра:
  
  .. code-block:: bash
  
    modinfo ./firstmod.ko
  
  На данном этапе, у нас урезанная система в QEMU. Но вы можете собрать модуль из примера
  на гостевой системе (он ведь работает, мы это проверили), выполнив 
  
  .. code-block:: bash
     
     make KBUILDDIR="/lib/modules/$(uname -r)/build/" modules
  
  (не забывайте делать это в чистой директории сборки, и почистить ее после проверки)
  
  Пример вывода:
  
  .. code-block::
  
    [thd@aspire lab1]$ modinfo ./firstmod.ko
    filename:       /home/thd/Work/KPI_Embedded_Linux/kernel/lab1/./firstmod.ko
    license:        Dual MIT/GPL
    version:        0.1
    author:         thodnev
    description:    Basic module demo: init, deinit, printk, jiffies
    srcversion:     E36B5885188779177077932
    depends:        
    retpoline:      Y
    name:           firstmod
    vermagic:       5.4.60-2-MANJARO SMP preempt mod_unload modversions 
 
  Отсюда видно, зачем мы использовали в коде ядра макросы вроде ``MODULE_AUTHOR``
  для определения служебных данных.
  
  А декларация ``MODULE_LICENSE`` вообще имеет особый смысл: если модуль имеет
  лицензию, несовместимую с GPL, то при его загрузке, ему не будут доступно множество
  функционала ядра; ядро при этом станет *tainted* (испорченным). Багрепорты с логами
  с такого tainted-ядра откажутся принимать. Таким образом обеспечивается политика
  мягкого принуждения к соблюдению политики open-source разработки ядра.
  
- Разработка модулей отличается от разработки пользовательских приложений.
  Это связано с тем, что модули ядра работают в пространстве ядра, а не в
  user space. Некоторые особенности:
  
  
  * Модули ядра являются привелегированными по отношению к юзерспейс-приложениям. 
    Бесконечный цикл или ожидание освобождения ресурса в цикле недопустимы и могут
    повесить всю систему (не многоядерную или без preemption, о котором поговорим
    позже) или существенно снизить ее производительность. Поэтому модули ядра 
    должны быть быстрыми. Особое внимание уделяется возможности/невозможности
    deadlock в блокировках доступа к разделяемым ресурсам;
  
  * Модули ядра не содержат ``main()`` и не выполняются линейно. При подключении
    модуля, он линкуется к ядру и запускается функция инициализации модуля. 
    Эта функция выполняет необходимые инициализации и *завершается, а модуль
    продолжает работать*;
    
  * Модули ядра не используют стандартных библиотек и соответствующих вызовов.
    В случае особой необходимости, использовать все же можно
    (ядро имеет доступ ко всему), но такая необходимость возникает редко, и 
    их использование в любом случае будет выглядеть иначе, 
    чем в юзерспейс-приложениях;
    
  * В следствие предыдущего пункта ``printf`` (а также другие привычные
    библиотечные функции) недоступны. Но у них есть
    `аналоги <https://www.kernel.org/doc/html/latest/core-api/kernel-api.html?highlight=print#basic-c-library-functions>`__
    из кода ядра.
    
    Вместо ``printf`` чаще всего используют `printk <https://www.kernel.org/doc/html/latest/core-api/printk-basics.html>`__;
    
  * Модули ядра не могут 
    `так же просто <https://stackoverflow.com/a/47056242/5750172>`__
    работать с числами с плавающей запятой,
    как это делают user space приложения. Это связано с особенностями FPU.
    Когда работает юзерспейс-приложение, возможность задействовать FPU ему
    предоставляет ядро. При работе в kernel space,  модуль должен
    самостоятельно обеспечить доступность FPU на время проведения операций.
    
    Чаще всего использование операций с плавающей запятой в коде ядра и модулей
    сводится к обрамлению блока кода ``kernel_fpu_begin()`` и ``kernel_fpu_end()``.
    Но это неэффективно и не портабельно (Linux ядро ведь может работать и 
    на архитектурах без FPU).
    
    Разработчики стараются всеми возможными способами избежать задержек, связанных
    с использованием FPU. Например, при помощи fixed point арифметики. Также модуль
    ядра может использовать `софтверную реализацию floating-point операций;
    
  * Модули ядра не освобождают ресурсы автоматически. Все выделенные ресурсы
    должны освобождаться вручную. При этом, важно предусмотреть нормальную
    отработку ситуаций, когда на этапе выделения определенного ресурса
    возникает ошибка;
    
  * Модули ядра могут прерываться, и делают это в разы чаще многопоточных
    userspace-приложений. При написании кода следует уделять этому особое
    внимание, **задумываясь, а что будет, если между двумя идущими друг
    за дружкой инструкциями произойдет передача управления**.


Задание
~~~~~~~

#. Выполните приведенные выше шаги;
#. Настало время начинать разбираться с особенностями базовых
   структур ядра и их организации:
   
   * Изучите документацию по `средствам вывода сообщений <https://www.kernel.org/doc/html/latest/core-api/printk-basics.html>`__
     (printk и более современные аналоги), а также по
     `формат-строке printk и аналогов <https://www.kernel.org/doc/html/latest/core-api/printk-formats.html>`__.
     Попробуйте использовать их для логгинга с разным уровнем важности;
     
   * В примере модуля был использован jiffies. Изучите `раздел LKD
     <http://books.gigatux.nl/mirror/kerneldevelopment/0672327201/ch10lev1sec3.html>`__
     об устройстве jiffies, а также `раздел Linux Insides
     <https://0xax.gitbooks.io/linux-insides/content/Timers/linux-timers-1.html#acquainted-with-jiffies>`__.
     Дополнительно можно заглянуть в исходники `include/linux/jiffies.h
     <https://github.com/torvalds/linux/blob/master/include/linux/jiffies.h>`__
     и `kernel/time/jiffies.c
     <https://github.com/torvalds/linux/blob/master/kernel/time/jiffies.c>`__.
   
#. Изучите раздел LKMPG `о макросах __init и __exit
   <https://tldp.org/LDP/lkmpg/2.6/html/x245.html>`__,
   а также `о передаче параметров модулю при загрузке
   <https://tldp.org/LDP/lkmpg/2.6/html/x323.html>`__;
   
#. Измените модуль ядра firstmod.c `из примера выше <#пример-модуля-ядра>`__,
   чтобы вместо ``$username``, выводилось имя, переданное в качестве параметра
   модуля при его подключении с помощью ``insmod``, а если параметр
   не задан – использовалось то же ``$username`` по-умолчанию и в лог
   ядра выводилось сообщения с уровнем логгинга WARNING, что имя не задано.

   *Примечание*:
     Параметр обязательно должен иметь описание `MODULE_PARM_DESC <https://www.tldp.org/LDP/lkmpg/2.6/html/x323.html>`__
   
   Дополните exit-callback модуля, чтобы при завершении модуль также
   печатал в лог текущее значение jiffies и время в секундах, которое
   прошло от init до exit (рассчитать из jiffies, используя
   `jiffies_delta_to_msecs
   <https://github.com/torvalds/linux/blob/master/include/linux/jiffies.h#L429>`__
   или `jiffies_to_timespec64
   <https://github.com/torvalds/linux/blob/34d4ddd359dbcdf6c5fb3f85a179243d7a1cb7f8/include/linux/jiffies.h#L421>`__).
   
   О том, что такое timespec64 можно почитать в `include/linux/time64.h
   <https://elixir.bootlin.com/linux/latest/source/include/linux/time64.h>`__.

#. Оформите протокол (в ReStructured Text или MarkDown), в котором необходимо описать:
   
   - краткую суть задания;
   - что было сделано, поэтапно и кратко;
   - полученные результаты и их интерпретация (почему, о чем говорит и т.д.);
   - какие выводы из этого следуют;
   
#. Выложите код модуля и Makefile для его сборки, а также протокол в виде
   Pull Request в GitHub-репозиторий курса
   (https://github.com/kpi-keoa/kpi-embedded-linux-course).
   
   Исходник модуля назовите по-своему. Это уже не первый модуль firstmod.
   Также большая просьба не указывать в ``KBUILDDIR`` выкладываемого в репозиторий
   Makefile свою конкретную директорию с исходниками ядра. Указывайте относительный
   путь, что-то нейтральное вроде ``../linux-5.8.7``
   
#. На защите будьте готовы отвечать на вопросы по использованным командам,
   особенностям вашего кода и кода из примеров, Makefile, а также основным
   изложенным здесь концепциям


====================================================
ЛР2: Средства отложенной работы: таймеры и тасклеты
====================================================

В предыдущей лабораторной мы рассмотрели базовую структуру модуля ядра и простейший механизм учета времени – `jiffies`.
В данной работе мы познакомимся с двумя механизмами выволнения отложенной работы – таймерами и тасклетами, а также
затронем механизмы динамической аллокации памяти в ядре.

Задание
~~~~~~~

#. Почитайте об аллокации памяти в ядре. Нас интересует `kmalloc()` (`kzalloc()` и другие), а также `kfree()`.
   В то же время, `vmalloc()` пока не затрагиваем:
   
   - `Раздел 8. Allocating Memory <https://bootlin.com/doc/books/ldd3.pdf#page=231>`__ LDD
     *(Linux Device Drivers by J. Corbet, A. Rubini, and G. Kroah-Hartman, 3rd ed., 2005)*.
   - Документ `Memory Allocation Guide из Core API <https://www.kernel.org/doc/html/latest/core-api/memory-allocation.html>`__.
     В нем есть активные ссылки, по которым можно почитать описание соответствующих функций
   - Описания функций slab-аллокатора:
     `The Slab Cache из Memory Management API <https://www.kernel.org/doc/html/latest/core-api/mm-api.html#the-slab-cache>`__.
   - О GFP-флагах и их комбинациях:
     `Memory Allocation Controls из Memory Management API <https://www.kernel.org/doc/html/latest/core-api/mm-api.html#memory-allocation-controls>`__
   
   Убедитесь, что вам понятно, например, когда стоит использовать флаг ``GFP_KERNEL``, а когда – его комбинацию с ``GFP_ATOMIC``.
   
   Дополнительно, помимо LDD, о механизмах аллокации памяти можно почитать в
   `разделе 12. Memory Management <https://www.doc-developpement-durable.org/file/Projets-informatiques/cours-&-manuels-informatiques/Linux/Linux%20Kernel%20Development,%203rd%20Edition.pdf#page=258>`__ LKD
   *(Linux Kernel Development by R. Love. , 3rd ed., 2010)*.
   
   Эти две книги доступны на `Google Drive курса <https://drive.google.com/drive/folders/1ah8Mn_N48faQvv5SW3pQENf437MI2Djy?usp=sharing>`__.
   
#. Почитайте о таймерах:
   
   - `Kernel Timers <http://www.makelinux.net/ldd3/?u=chp-7-sect-4.shtml>`__ раздела *7. Time, Delays, and Deferred Work* LDD.
     О небольших изменениях в API таймеров с момента выхода LDD можно почитать в статье
     `Improving the kernel timers API на LWN <https://lwn.net/Articles/735887>`__.
   - `Timers <https://www.doc-developpement-durable.org/file/Projets-informatiques/cours-&-manuels-informatiques/Linux/Linux%20Kernel%20Development,%203rd%20Edition.pdf#page=249>`__ раздела *11. Timers and Time Management* LKD.
   - Найдите описание соответствующих функций в `подразделе Delaying, scheduling, and timer routines
     документации Driver API <https://www.kernel.org/doc/html/latest/driver-api/basics.html?#delaying-scheduling-and-timer-routines>`__.
   - При необходимости, обратитесь к исходному коду заголовка
     `linux/timer.h <https://github.com/torvalds/linux/blob/master/include/linux/timer.h>`__ и его имплементации
     `kernel/time/timer.c <https://github.com/torvalds/linux/blob/master/kernel/time/timer.c>`__.
   
#. Почитайте о тасклетах:
   
   - `Tasklets <http://www.makelinux.net/ldd3/?u=chp-7-sect-5.shtml>`_ раздела *7. Time, Delays, and Deferred Work* LDD.
   - Краткое описание `Softirqs and Tasklets
     <https://www.doc-developpement-durable.org/file/Projets-informatiques/cours-&-manuels-informatiques/Linux/Linux%20Kernel%20Development,%203rd%20Edition.pdf#page=163>`__
     , а также подраздел `Tasklets
     <https://www.doc-developpement-durable.org/file/Projets-informatiques/cours-&-manuels-informatiques/Linux/Linux%20Kernel%20Development,%203rd%20Edition.pdf#page=169>`__
     раздела *8. Bottom Halves and Deferring Work* LKD.

#. Напишите модуль ядра, который:
   
   - Принимает два параметра, ``cnt`` и ``delay``:
     
     * ``cnt`` является количеством циклов, которые должен отработать таймер;
     * ``delay`` является задержкой между двумя срабатываниями таймера.
     
     Примечание:
       Модуль должен отрабатывать при ``cnt`` и ``delay`` равных нулю.
   
   - В начале *init* печатает текущее значение ``jiffies`` в лог ядра.
   
   - Затем запускает тасклет, который должен напечатать свое значение ``jiffies`` в лог ядра.
     
     Примечание:
       В формат-строках указывайте, кто печатает в лог, чтобы не запутаться
     
   - Затем выделяет массив размера ``cnt``, используя динамическую аллокацию.
     В этот массив таймер будет складывать значения.
     
     Примечание:
       ``kmalloc()``, ``kzalloc()`` и аналоги возвращают ``void *``.
       Стандарт С гарантирует, что ``void *`` может быть приведен
       к любому типу указателя автоматически. Поэтому, результат
       этих функций **никогда** не нужно кастовать к типу указателя.
       
       Аллокации памяти должны быть замкнуты на имя переменной, а не на тип.
       Поэтому, вместо ``sizeof(int)``, нужно использовать ``sizeof(*myvar)``.
       
       Более подробно это описано в `README.rst <README.rst>`__ репозитория.
   
   - Затем *init* запускает таймер с задержкой ``delay`` и функция завершается.
     
     При срабатывании, таймер кладет текущее значение ``jiffies`` в массив и 
     перезапускается с задержкой ``delay``. Общее количество раз, которые
     запускается таймер, равно ``cnt``
   
   - В *exit*, модуль должен напечатать текущее значение ``jiffies``,
     а также вывести все значения из массива.
     
     В случае, если *exit* был вызван до того, как таймер успел отработать
     ``cnt`` раз, необходимо отменить выполнение последующего запланированного
     запуска таймера, вывести в лог ядра сообщение о досрочной выгрузке модуля
     и напечатать в лог те элементы массива, которые успел заполнить таймер.
   
   
   Примечание:
     Помните, что простые (basic) таймеры и тасклеты реализованы через softirq.
     Это значит, что они работают в атомарном контексте. В следствие этого,
     они не могут спать, запускать планирование задач и должны быть быстрыми.
     
     Если простой таймер или тасклет вызывает API ядра, которое может спать
     (например, `kmalloc()` с неверными GFP-флагами), то вызов заставит ожидать события,
     (в этом примере, пока аллокатор не будет готов выделить память).
     В то же время, пока функция простого таймера или тасклета не завершиться,
     нужный компонент ядра не сможет получить управления. В результате, это
     эквивалентно deadlock (ведет себя как бесконечный цикл ``while (1);``
     в вызывающей функции).
     Ничего хорошего из этого не выйдет.
     Планировщик ядра не получит управление до следующего тика, в результате
     выстрелят watchdog'и.
     
     Поэтому, в атомарном контексте нельзя спать. Например, при аллокациях
     памяти, необходимо использовать соответствующие флаги.
     
     Помните, что аллокатор может не найти свободной памяти в общем пуле для
     выделения в момент запуска из softirq и вернет ошибку. Если память все
     же необходимо выделить (например, чтобы не пропустить событие)
     – необходимо использовать экстренные (emergency) пулы зарезервированной
     памяти. Например, флаг ``GFP_ATOMIC``.
  
   Об атомарности:
     Неплохо бы позаботится об атомарном доступе, как минимум, к переменной-счетчику,
     используемой в функции таймера. У нас довольно простой случай: один writer и один reader,
     поэтому, проблем с синхронизацией возникать не должно.
     Тем не мене, возможна ситуация, когда таймер еще не успел обновить значение счетчика,
     а *exit* уже был вызван. Поэтому, в *exit* необходимо сперва завершать (или дожидаться
     завершения) таймера, а лишь затем вычитывать значение счетчика.
     
     Мы не закрываем массив со значениями локом. Но из-за того, что в один момент
     времени только один таймер может в него писать, а также (при использовании описанной
     в предыдущем абзаце процедуры) все таймеры закончат работу до момента чтения значений
     массива, проблем не возникнет.
     
     О механизмах синхронизации детально поговорим на следующих занятиях.
   
   Об обработке исключений:
     Мы работаем с ядром. Не нужно полагать, что вызов завершится успешно.
     Вместо этого, нужно каждый раз проверять возвращаемый результат и
     предпринимать соответствующие действия при возникновении ошибки.
     
     Деаллокации выполняются в порядке обратном аллокациям. Например:
     
     .. code-block:: c
        
        int status = 0;		// ok
	
	int *a = kzalloc(sizeof(*a), GFP_KERNEL);
	if (NULL == a) {
	        status = -ENOMEM;
	        goto final;		// << NOTICE ORDER HERE
        }
	
	if ((b = kzalloc(sizeof(*b), GFP_KERNEL)) == NULL) {  // spartan style
	        status = -ENOMEM;
	        goto dealloc_a;		// << NOTICE ORDER HERE
        }
	
	unsigned char *arr = kmalloc(100 * sizeof(*arr), GFP_KERNEL);
	if (NULL == arr) {  // spartan style
	        status = -ENOMEM;
	        goto dealloc_b;		// << NOTICE ORDER HERE
        }
	
	process(a, b, arr);
	
	
	dealloc_arr:
	        kfree(arr);
	dealloc_b:
		kfree(b);
	dealloc_a:
		kfree(a);
	final:
	
	return status;
     
     В качестве альтернативы (особенно, если аллокация и деаллокация выполняются в разных фукнциях,
     как *init()* и *exit()* модуля), можно завести структуру – битовое поле (bit field),
     где каждый бит будет устанавливатья в 1 после успешной аллокации.
     И функцию, которая будет сканировать биты в обратном порядке, выполняя деаллокацию.

#. Оформите протокол (в ReStructured Text или MarkDown).
   В протоколе необходимо описать
   
   - краткую суть задания;
   - что было сделано, поэтапно и кратко;
   - полученные результаты и их интерпретация (почему, о чем говорит и т.д.);
   - какие выводы из этого следуют;
   
   а также, в протоколе необходимо дать ответ с кратким объяснением на следующие вопросы:
   
   - Почему ``jiffies``, печатаемое в *init* и ``jiffies``, выводимое тасклетом могут
     отличаться на 0, 1 или 2? Объясните для каждого случая.
   - Какие GFP-флаги использовались и почему именно они?
   - Чему равна разница ``jiffies`` между двумя запусками таймера и почему?
   - Что произойдет при задании параметра ``delay`` равным нулю?

#. Выложите код модуля и Makefile для его сборки, а также протокол в виде
   Pull Request в GitHub-репозиторий курса
   (https://github.com/kpi-keoa/kpi-embedded-linux-course)
   
.. #. Ознакомьтесь со связными списками ядра.
   Прочитайте `главу LDD <http://www.makelinux.net/ldd3/?u=chp-11-sect-5.shtml>`__ по спискам,
   раздел `Linux Insides <https://0xax.gitbooks.io/linux-insides/content/DataStructures/linux-datastructures-1.html>`__
   и соответствующую `документацию Kernel API <https://www.kernel.org/doc/html/latest/core-api/kernel-api.html>`__.
   
   Посмотрите исходный код `linux/list.h <https://github.com/torvalds/linux/blob/master/include/linux/list.h>`__.
   
   Дополнительно, можно прочитать краткий `FAQ на Kernel Newbies <https://kernelnewbies.org/FAQ/LinkedLists>`__.
   Убедитесь, что вы разобрались с устройством linked lists в ядре прежде, чем приступать к дальнейшей работе.

